{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROJECT CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASKS:\n",
    "\n",
    "1_Expansão com hidrolases - MEME and BLAST\n",
    "2_Features - NS Machine learning - Clustering and Kmers\n",
    "3_Functional Analysis - MOFA \n",
    "4_Estructural - Ensemble \n",
    "\n",
    "O que posso aplicar para a generalidade dos casos?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blast\n",
    "\n",
    "from Bio.Blast.Applications import NcbiblastpCommandline\n",
    "\n",
    "# Arquivos de entrada\n",
    "initial_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta\"\n",
    "new_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/new_sequences.fasta\"\n",
    "output_file = \"/Users/Sergiomendes/Desktop/Project/data/blast_results.tsv\"\n",
    "\n",
    "# Remover o arquivo de saída, se existir\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Executar o BLAST usando o package do Anaconda e gravar os resultados no arquivo blast_results.tsv\n",
    "blastp_cline = NcbiblastpCommandline(query=new_sequences_file, subject=initial_sequences_file, outfmt=6, out=output_file)\n",
    "stdout, stderr = blastp_cline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise MEME concluída. Os resultados foram salvos no arquivo meme_results.txt.\n"
     ]
    }
   ],
   "source": [
    "#MEME\n",
    "\n",
    "# Arquivos de entrada\n",
    "initial_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta\"\n",
    "new_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/new_sequences.fasta\"\n",
    "output_file = \"/Users/Sergiomendes/Desktop/Project/data/meme_results.txt\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Comando para executar o programa meme e redirecionar a saída para o arquivo\n",
    "command = [\"/opt/local/bin/meme\", initial_sequences_file, new_sequences_file, \"-nmotifs\", \"3\", \"-oc\", output_file]\n",
    "\n",
    "# Executar o comando\n",
    "subprocess.run(command)\n",
    "\n",
    "print(\"Análise MEME concluída. Os resultados foram salvos no arquivo meme_results.txt.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-70bbc411f0b0>:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data = np.array(padded_sequences, dtype=np.float)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "initial_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta\"\n",
    "new_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/new_sequences.fasta\"\n",
    "\n",
    "def read_fasta(file_path):\n",
    "    sequences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        sequence = ''\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if sequence:\n",
    "                    sequences.append(sequence)\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line\n",
    "        if sequence:\n",
    "            sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "# Retornar os encoders de cada sequencia\n",
    "def encode_sequence(sequence):\n",
    "    encoding = []\n",
    "    for amino_acid in sequence:\n",
    "        encoding.append(ord(amino_acid))\n",
    "    return encoding\n",
    "\n",
    "file_paths = [initial_sequences_file, new_sequences_file]\n",
    "\n",
    "encoders_file1 = []\n",
    "encoders_file2 = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    sequences = read_fasta(file_path)\n",
    "    encoders = []\n",
    "    for sequence in sequences:\n",
    "        encoding = encode_sequence(sequence)\n",
    "        encoders.append(encoding)\n",
    "\n",
    "    if file_path == initial_sequences_file:\n",
    "        encoders_file1 = encoders\n",
    "    elif file_path == new_sequences_file:\n",
    "        encoders_file2 = encoders\n",
    "\n",
    "all_encoders = encoders_file1 + encoders_file2\n",
    "\n",
    "# Preencher as sequências com zeros para terem o mesmo comprimento\n",
    "padded_sequences = pad_sequences(all_encoders, padding='post')\n",
    "\n",
    "# Converter a lista de encoders em uma matriz numpy de ponto flutuante\n",
    "data = np.array(padded_sequences, dtype=np.float)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "clusters = kmeans.fit_predict(principal_components)\n",
    "\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=clusters)\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.title('Clustering usando K-means')\n",
    "plt.savefig('clustering_result.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# PCA & unsupervised K-means (A adaptar)\n",
    "\n",
    "# resfinder matriz 01\n",
    "tudodegenes = []\n",
    "for row in range(len(comb_file['resfinder'])):\n",
    "    rf_ls = comb_file['resfinder'][row]\n",
    "    tudodegenes.append(rf_ls)\n",
    "flat_tudodegenes = [x for xs in tudodegenes for x in xs]\n",
    "final_tg = list(dict.fromkeys(flat_tudodegenes))\n",
    "\n",
    "# add all zeros\n",
    "for i in final_tg:\n",
    "    comb_file.insert(loc=9, column=i, value=0)\n",
    "# if hit = 1\n",
    "final_resfinder = comb_file.set_index('id')\n",
    "id_rf = dict(zip(comb_file.id, final_resfinder['resfinder']))\n",
    "for key, value in id_rf.items():\n",
    "    for i in value:\n",
    "        for z in final_resfinder.columns[8:]:\n",
    "            if i == z:\n",
    "                final_resfinder.loc[key, z] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "# standardização dos dados\n",
    "scaler = StandardScaler()\n",
    "final_resfinder_scaled = scaler.fit_transform(final_resfinder[final_tg])\n",
    "\n",
    "# Pca\n",
    "rf_PCA = PCA()\n",
    "rf_PCA.fit(final_resfinder[final_tg])\n",
    "rf_pca_reduced = rf_PCA.transform(final_resfinder[final_tg])\n",
    "\n",
    "\n",
    "for x in final_resfinder[\"type\"].unique():\n",
    "    sp = comb_file.index[final_resfinder[\"type\"] == x]\n",
    "    if x == \"Agricultural\" or x == \"Freshwater\":\n",
    "        plt.plot(rf_pca_reduced[sp, 0], rf_pca_reduced[sp, 1], \"X\", label=x, markersize=8)\n",
    "    else:\n",
    "        plt.plot(rf_pca_reduced[sp, 0], rf_pca_reduced[sp, 1], \"o\", label=x, markersize=8)\n",
    "\n",
    " add dashed lines for zero lines\n",
    "plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.title(\"ResFinder: ARGs PCA\")\n",
    "plt.xlabel(\"P.C. 1: \" + str(rf_PCA.explained_variance_ratio_[0])[0:4])\n",
    "plt.ylabel(\"P.C. 2: \" + str(rf_PCA.explained_variance_ratio_[1])[0:4])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# set x-axis limits\n",
    "plt.xlim([-6, 16])\n",
    "plt.xticks(range(-5, 16, 5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# k means clustering resfinder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = final_resfinder[final_tg]\n",
    "pca = PCA(2)\n",
    "df_1 = pca.fit_transform(data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, max_iter=1000)\n",
    "label = kmeans.fit_predict(df_1)\n",
    "centroids = kmeans.cluster_centers_\n",
    "u_labels = np.unique(label)\n",
    "for i in u_labels:\n",
    "    plt.scatter(df_1[label == i, 0], df_1[label == i, 1], label=i)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=80, color='k')\n",
    "plt.title(\"Resfinder kmeans\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# optimal kmeans resfinder\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1, 15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df_1)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k: Resfinder')\n",
    "plt.show()\n",
    "\n",
    "final_resfinder.to_excel(\"C:/Users/diogo/Desktop/rf_01.xlsx\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONAL ANALYSIS - MOFA\n",
    "\n",
    "import numpy as np\n",
    "from mofapy2.mofapy2 import MOFA\n",
    "from Bio import SeqIO\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Carregar as sequências de proteínas do arquivo initial_sequences.fasta\n",
    "initial_sequences = []\n",
    "with open(inicial_sequences_file, \"r\") as initial_seq_file:\n",
    "    for record in SeqIO.parse(initial_seq_file, \"fasta\"):\n",
    "        initial_sequences.append(str(record.seq))\n",
    "\n",
    "# Carregar as sequências de proteínas do arquivo new_sequences.fasta\n",
    "new_sequences = []\n",
    "with open(new_sequences_file, \"r\") as new_seq_file:\n",
    "    for record in SeqIO.parse(new_seq_file, \"fasta\"):\n",
    "        new_sequences.append(str(record.seq))\n",
    "\n",
    "# Codificar as sequências de proteínas usando One-Hot Encoding\n",
    "encoder = OneHotEncoder(dtype=np.int8)\n",
    "protein_sequences = initial_sequences + new_sequences\n",
    "encoded_sequences = encoder.fit_transform(protein_sequences).toarray()\n",
    "\n",
    "# Preparar as características funcionais relevantes \n",
    "# Certificar que as características funcionais estão no formato adequado (variáveis numéricas ou categóricas)\n",
    "\n",
    "# Criar a matriz de dados para a análise MOFA\n",
    "X = np.array(encoded_sequences)\n",
    "\n",
    "# Configurar e executar o MOFA\n",
    "model = MOFA()\n",
    "model.set_data_matrix(X)\n",
    "model.fit(n_factors=3)  # Definir o número de fatores latentes desejado\n",
    "\n",
    "# Analisar os resultados do MOFA\n",
    "factors = model.get_factors()\n",
    "loadings = model.get_loadings()\n",
    "\n",
    "# Exemplo de impressão dos resultados\n",
    "for factor_idx, factor in enumerate(factors):\n",
    "    print(f\"Factor {factor_idx + 1}:\")\n",
    "    print(factor)\n",
    "    print(\"\")\n",
    "\n",
    "for loading_idx, loading in enumerate(loadings):\n",
    "    print(f\"Loading {loading_idx + 1}:\")\n",
    "    print(loading)\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRUCTURAL ANALYSIS - ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPANSÃO DO DATASET\n",
    "\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import requests\n",
    "\n",
    "# Define os termos de busca e filtra por proteínas da classe hidrolase\n",
    "search_term = 'hidrolase NOT \"PET degradation\"'\n",
    "search_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=protein&term={search_term}&retmax=100000'\n",
    "search_result = requests.get(search_url).text\n",
    "id_list = search_result.split('<Id>')[1:]\n",
    "id_list = [id.split('</Id>')[0] for id in id_list]\n",
    "\n",
    "# Baixa as sequências das proteínas encontradas\n",
    "fetch_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=protein&id={\",\".join(id_list)}&rettype=fasta&retmode=text'\n",
    "sequences = SeqIO.parse(requests.get(fetch_url).text, 'fasta')\n",
    "\n",
    "# Grava as sequências em um arquivo FASTA na pasta desejada\n",
    "output_folder = '/Users/Sergiomendes/Desktop/Project/Code/data'\n",
    "output_file = os.path.join(output_folder, 'proteins_not_degrading_pet.fasta')\n",
    "with open(output_file, 'w') as f:\n",
    "    SeqIO.write(sequences, f, 'fasta')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUNTAR OS FICHEIROS FASTA CRIANDO UM DATASET EXPANDIDO DE HIDROLASES\n",
    "\n",
    "# Define os caminhos para os arquivos FASTA que serão combinados\n",
    "file1 = '/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta'\n",
    "file2 = '/Users/Sergiomendes/Desktop/Project/data/proteins_not_degrading_pet.fasta'\n",
    "\n",
    "# Lê as sequências de ambos os arquivos\n",
    "seqs1 = SeqIO.parse(file1, 'fasta')\n",
    "seqs2 = SeqIO.parse(file2, 'fasta')\n",
    "\n",
    "# Combina as sequências em uma lista\n",
    "combined_seqs = list(seqs1) + list(seqs2)\n",
    "\n",
    "# Define o caminho para o arquivo de saída\n",
    "output_folder = '/Users/Sergiomendes/Desktop/Project/data'\n",
    "output_file = os.path.join(output_folder, 'expanded_dataset.fasta')\n",
    "\n",
    "# Grava as sequências combinadas em um único arquivo FASTA\n",
    "with open(output_file, 'w') as output_handle:\n",
    "    SeqIO.write(combined_seqs, output_handle, 'fasta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
