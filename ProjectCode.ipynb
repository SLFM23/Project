{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROJECT CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASKS:\n",
    "\n",
    "1_Expansão com hidrolases - MEME and BLAST\n",
    "2_Features - NS Machine learning - Clustering and Kmers\n",
    "3_Functional Analysis - MOFA \n",
    "4_Estructural - Ensemble \n",
    "\n",
    "O que posso aplicar para a generalidade dos casos?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4f3f1135110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Executa o BLAST para a sequência atual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mrun_blast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_sequences_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a4f3f1135110>\u001b[0m in \u001b[0;36mrun_blast\u001b[0;34m(query_sequence, database, output_file)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_blast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNCBIWWW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqblast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blastp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mblast_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNCBIXML\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/Bio/Blast/NCBIWWW.py\u001b[0m in \u001b[0;36mqblast\u001b[0;34m(program, database, sequence, url_base, auto_format, composition_based_statistics, db_genetic_code, endpoints, entrez_query, expect, filter, gapcosts, genetic_code, hitlist_size, i_thresh, layout, lcase_mask, matrix_name, nucl_penalty, nucl_reward, other_advanced, perc_ident, phi_pattern, query_file, query_believe_defline, query_from, query_to, searchsp_eff, service, threshold, ungapped_alignment, word_size, short_query, alignments, alignment_view, descriptions, entrez_links_new_window, expect_low, expect_high, format_entrez_query, format_object, format_type, ncbi_gi, results_file, show_overview, megablast, template_type, template_length)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqblast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_previous\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mqblast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#BLAST \n",
    "\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "def run_blast(query_sequence, database, output_file):\n",
    "    result_handle = NCBIWWW.qblast(\"blastp\", database, query_sequence)\n",
    "    blast_records = NCBIXML.parse(result_handle)\n",
    "\n",
    "    with open(output_file, \"a\") as output:\n",
    "        for blast_record in blast_records:\n",
    "            for alignment in blast_record.alignments:\n",
    "                for hsp in alignment.hsps:\n",
    "                    # Escreve os resultados no arquivo de saída\n",
    "                    output.write(f\"Query: {blast_record.query}\\n\")\n",
    "                    output.write(f\"Subject: {alignment.title}\\n\")\n",
    "                    output.write(f\"Score: {hsp.score}\\n\")\n",
    "                    output.write(f\"Alignment length: {hsp.align_length}\\n\")\n",
    "                    output.write(f\"E-value: {hsp.expect}\\n\")\n",
    "                    output.write(f\"Identity: {hsp.identities}\\n\")\n",
    "                    output.write(f\"Query sequence: {hsp.query}\\n\")\n",
    "                    output.write(f\"Subject sequence: {hsp.sbjct}\\n\\n\")\n",
    "\n",
    "# Arquivos de entrada\n",
    "initial_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta\"\n",
    "new_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/new_sequences.fasta\"\n",
    "output_file = \"/Users/Sergiomendes/Desktop/Project/data/blast_results.txt\"\n",
    "\n",
    "# Remover o arquivo de saída, se existir\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Executar o BLAST para cada sequência do arquivo new_sequences.fasta\n",
    "with open(new_sequences_file, \"r\") as new_seq_file:\n",
    "    for record in SeqIO.parse(new_seq_file, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        # Executa o BLAST para a sequência atual\n",
    "        run_blast(sequence, initial_sequences_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise MEME concluída. Os resultados foram salvos no arquivo meme_results.txt.\n"
     ]
    }
   ],
   "source": [
    "#MEME\n",
    "\n",
    "# Arquivos de entrada\n",
    "initial_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta\"\n",
    "new_sequences_file = \"/Users/Sergiomendes/Desktop/Project/data/new_sequences.fasta\"\n",
    "output_file = \"/Users/Sergiomendes/Desktop/Project/data/meme_results.txt\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Comando para executar o programa meme e redirecionar a saída para o arquivo\n",
    "command = [\"/opt/local/bin/meme\", initial_sequences_file, new_sequences_file, \"-nmotifs\", \"3\", \"-o\", output_file]\n",
    "\n",
    "# Executar o comando\n",
    "subprocess.run(command)\n",
    "\n",
    "print(\"Análise MEME concluída. Os resultados foram salvos no arquivo meme_results.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA & unsupervised K-means (A adaptar)\n",
    "\n",
    "# resfinder matriz 01\n",
    "tudodegenes = []\n",
    "for row in range(len(comb_file['resfinder'])):\n",
    "    rf_ls = comb_file['resfinder'][row]\n",
    "    tudodegenes.append(rf_ls)\n",
    "flat_tudodegenes = [x for xs in tudodegenes for x in xs]\n",
    "final_tg = list(dict.fromkeys(flat_tudodegenes))\n",
    "\n",
    "# add all zeros\n",
    "for i in final_tg:\n",
    "    comb_file.insert(loc=9, column=i, value=0)\n",
    "# if hit = 1\n",
    "final_resfinder = comb_file.set_index('id')\n",
    "id_rf = dict(zip(comb_file.id, final_resfinder['resfinder']))\n",
    "for key, value in id_rf.items():\n",
    "    for i in value:\n",
    "        for z in final_resfinder.columns[8:]:\n",
    "            if i == z:\n",
    "                final_resfinder.loc[key, z] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "# standardização dos dados\n",
    "scaler = StandardScaler()\n",
    "final_resfinder_scaled = scaler.fit_transform(final_resfinder[final_tg])\n",
    "\n",
    "# Pca\n",
    "rf_PCA = PCA()\n",
    "rf_PCA.fit(final_resfinder[final_tg])\n",
    "rf_pca_reduced = rf_PCA.transform(final_resfinder[final_tg])\n",
    "\n",
    "\n",
    "for x in final_resfinder[\"type\"].unique():\n",
    "    sp = comb_file.index[final_resfinder[\"type\"] == x]\n",
    "    if x == \"Agricultural\" or x == \"Freshwater\":\n",
    "        plt.plot(rf_pca_reduced[sp, 0], rf_pca_reduced[sp, 1], \"X\", label=x, markersize=8)\n",
    "    else:\n",
    "        plt.plot(rf_pca_reduced[sp, 0], rf_pca_reduced[sp, 1], \"o\", label=x, markersize=8)\n",
    "\n",
    " add dashed lines for zero lines\n",
    "plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.title(\"ResFinder: ARGs PCA\")\n",
    "plt.xlabel(\"P.C. 1: \" + str(rf_PCA.explained_variance_ratio_[0])[0:4])\n",
    "plt.ylabel(\"P.C. 2: \" + str(rf_PCA.explained_variance_ratio_[1])[0:4])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# set x-axis limits\n",
    "plt.xlim([-6, 16])\n",
    "plt.xticks(range(-5, 16, 5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# k means clustering resfinder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = final_resfinder[final_tg]\n",
    "pca = PCA(2)\n",
    "df_1 = pca.fit_transform(data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, max_iter=1000)\n",
    "label = kmeans.fit_predict(df_1)\n",
    "centroids = kmeans.cluster_centers_\n",
    "u_labels = np.unique(label)\n",
    "for i in u_labels:\n",
    "    plt.scatter(df_1[label == i, 0], df_1[label == i, 1], label=i)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=80, color='k')\n",
    "plt.title(\"Resfinder kmeans\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# optimal kmeans resfinder\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1, 15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df_1)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k: Resfinder')\n",
    "plt.show()\n",
    "\n",
    "final_resfinder.to_excel(\"C:/Users/diogo/Desktop/rf_01.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONAL ANALYSIS - MOFA\n",
    "\n",
    "import numpy as np\n",
    "from mofapy2.mofapy2 import MOFA\n",
    "from Bio import SeqIO\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Carregar as sequências de proteínas do arquivo initial_sequences.fasta\n",
    "initial_sequences = []\n",
    "with open(inicial_sequences_file, \"r\") as initial_seq_file:\n",
    "    for record in SeqIO.parse(initial_seq_file, \"fasta\"):\n",
    "        initial_sequences.append(str(record.seq))\n",
    "\n",
    "# Carregar as sequências de proteínas do arquivo new_sequences.fasta\n",
    "new_sequences = []\n",
    "with open(new_sequences_file, \"r\") as new_seq_file:\n",
    "    for record in SeqIO.parse(new_seq_file, \"fasta\"):\n",
    "        new_sequences.append(str(record.seq))\n",
    "\n",
    "# Codificar as sequências de proteínas usando One-Hot Encoding\n",
    "encoder = OneHotEncoder(dtype=np.int8)\n",
    "protein_sequences = initial_sequences + new_sequences\n",
    "encoded_sequences = encoder.fit_transform(protein_sequences).toarray()\n",
    "\n",
    "# Preparar as características funcionais relevantes \n",
    "# Certificar que as características funcionais estão no formato adequado (variáveis numéricas ou categóricas)\n",
    "\n",
    "# Criar a matriz de dados para a análise MOFA\n",
    "X = np.array(encoded_sequences)\n",
    "\n",
    "# Configurar e executar o MOFA\n",
    "model = MOFA()\n",
    "model.set_data_matrix(X)\n",
    "model.fit(n_factors=3)  # Definir o número de fatores latentes desejado\n",
    "\n",
    "# Analisar os resultados do MOFA\n",
    "factors = model.get_factors()\n",
    "loadings = model.get_loadings()\n",
    "\n",
    "# Exemplo de impressão dos resultados\n",
    "for factor_idx, factor in enumerate(factors):\n",
    "    print(f\"Factor {factor_idx + 1}:\")\n",
    "    print(factor)\n",
    "    print(\"\")\n",
    "\n",
    "for loading_idx, loading in enumerate(loadings):\n",
    "    print(f\"Loading {loading_idx + 1}:\")\n",
    "    print(loading)\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRUCTURAL ANALYSIS - ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPANSÃO DO DATASET\n",
    "\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import requests\n",
    "\n",
    "# Define os termos de busca e filtra por proteínas da classe hidrolase\n",
    "search_term = 'hidrolase NOT \"PET degradation\"'\n",
    "search_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=protein&term={search_term}&retmax=100000'\n",
    "search_result = requests.get(search_url).text\n",
    "id_list = search_result.split('<Id>')[1:]\n",
    "id_list = [id.split('</Id>')[0] for id in id_list]\n",
    "\n",
    "# Baixa as sequências das proteínas encontradas\n",
    "fetch_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=protein&id={\",\".join(id_list)}&rettype=fasta&retmode=text'\n",
    "sequences = SeqIO.parse(requests.get(fetch_url).text, 'fasta')\n",
    "\n",
    "# Grava as sequências em um arquivo FASTA na pasta desejada\n",
    "output_folder = '/Users/Sergiomendes/Desktop/Project/Code/data'\n",
    "output_file = os.path.join(output_folder, 'proteins_not_degrading_pet.fasta')\n",
    "with open(output_file, 'w') as f:\n",
    "    SeqIO.write(sequences, f, 'fasta')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUNTAR OS FICHEIROS FASTA CRIANDO UM DATASET EXPANDIDO DE HIDROLASES\n",
    "\n",
    "# Define os caminhos para os arquivos FASTA que serão combinados\n",
    "file1 = '/Users/Sergiomendes/Desktop/Project/data/initial_sequences.fasta'\n",
    "file2 = '/Users/Sergiomendes/Desktop/Project/data/proteins_not_degrading_pet.fasta'\n",
    "\n",
    "# Lê as sequências de ambos os arquivos\n",
    "seqs1 = SeqIO.parse(file1, 'fasta')\n",
    "seqs2 = SeqIO.parse(file2, 'fasta')\n",
    "\n",
    "# Combina as sequências em uma lista\n",
    "combined_seqs = list(seqs1) + list(seqs2)\n",
    "\n",
    "# Define o caminho para o arquivo de saída\n",
    "output_folder = '/Users/Sergiomendes/Desktop/Project/data'\n",
    "output_file = os.path.join(output_folder, 'expanded_dataset.fasta')\n",
    "\n",
    "# Grava as sequências combinadas em um único arquivo FASTA\n",
    "with open(output_file, 'w') as output_handle:\n",
    "    SeqIO.write(combined_seqs, output_handle, 'fasta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
